import json
from typing import List, Dict
import time
import sys
import re
from wolframclient.evaluation import WolframLanguageSession
from wolframclient.language import wl, wlexpr
from wolframclient.exception import WolframLanguageException
from hyh.embedding import rag
from collections import OrderedDict
from copy import deepcopy
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import os

class ModelFunc:
    def __init__(self, model_path: str):

        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")

        # self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.device = torch.device("cuda:2")
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
            self.model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True).to(self.device)
            self.model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        except Exception as e:
            raise RuntimeError(f"åŠ è½½æ¨¡å‹æˆ–åˆ†è¯å™¨æ—¶å‡ºé”™: {e}")

    def generate(self, prompt: str, max_tokens) -> str:

        try:
            with torch.no_grad():
                message = [{"role":"user","content":prompt}]
                conversation = self.tokenizer.apply_chat_template(message,add_generation_prompt=True,tokenize=False)
                encoading = self.tokenizer(conversation,return_tensors="pt").to(self.device)
                output_ids = self.model.generate(**encoading,max_new_tokens=max_tokens,repetition_penalty=1.1,pad_token_id=self.tokenizer.eos_token_id)
                response = self.tokenizer.decode(output_ids[0])
                return response
        except Exception as e:
            raise RuntimeError(f"ç”Ÿæˆå›å¤æ—¶å‡ºé”™: {e}")
        
class WorkflowExecutor:
    def __init__(self,model_path):
        self.sub_problems = []
        self.intermediate_answers = []
        self.final_conclusion = ""
        self.mma = WolframLanguageSession()
        self.problem = ""
        self.model = ModelFunc(model_path=model_path)
    
    def eval_split_result(self,response):
        eval_prompt = """
Your task is to evaluate the quality of the following response.
If the response is perfect and does not require any changes, reply with the word "perfect".
If the response has issues that can be fixed, reply with the word "fixable".
Here is the response: {problem}"""

        response = self.model.generate(
            eval_prompt.format(problem=response),
            max_tokens= 1000
        )
        if "perfect" in response:
            return "perfect", response  
        else:
            return "fixable", response

    def eval_final_result(self,response):
        eval_prompt = ""
        response = self.generate(
        eval_prompt.format(problem=response),
        max_tokens= 1000
        )
        return response  

    # é˜¶æ®µ1: é—®é¢˜æ‹†åˆ†
    def split_into_subproblems(self) -> List[str]:
        max_attempts = 5  # é˜²æ­¢æ— é™å¾ªç¯
        error_history = []  # è®°å½•é”™è¯¯å†å²
        error_feedback = ""
        system_prompt = """
You are a physics expert. Please solve the following problem. 
You can solve the problem or break it down into series subproblems when the problem is complex.
If you can solve it directly, please use \\box{{}} around your final result.
If you decide to break it down, pay attention to:
- Some problems can be broken down into four steps: "rigorously analyze the geometric and physical context", "list equations and transform them into mathematical problems", "solve the equations", and "get conclusions from the results".
- Ignore details that are not relevant to solving the problem, such as unnecessary object properties or unrelated parts of the system.
- If applicable, check the limiting behaviors or simplified cases to see if the problem can be reduced further. This can be the last subproblem.
- Use strict JSON array format: ["Sub-problem 1", "Sub-problem 2", "Sub-problem 3"].
- Avoiding unnecessary subdivisions.

{error_feedback}
Problem: {problem}
<think>"""
        
        for attempt in range(max_attempts):
            # ç”Ÿæˆå¸¦é”™è¯¯åé¦ˆçš„æç¤º
            error_feedback = "\n".join(error_history[-2:])  # åªæ˜¾ç¤ºæœ€è¿‘ä¸¤æ¬¡é”™è¯¯
            prompt = system_prompt.format(
                problem=self.problem,
                error_feedback=f"Previous issues to be solved:\n{error_feedback}" if error_feedback else ""
            )
            
            # è°ƒç”¨æ¨¡å‹ç”Ÿæˆ
            response = self.model.generate(prompt, max_tokens=10000)
            
            print("-----------------------",response,"-----------------------")
            
            
            # è¯„ä¼°å¹¶è·å–è¯¦ç»†åé¦ˆ
            evaluation, feedback = self.eval_split_result(response)
            
            if evaluation == "perfect":
                break
                # return parse_json(response)  # å‡è®¾æœ‰è§£æJSONçš„å‡½æ•°
            else:
                error_history.append(f"Attempt {attempt+1} failed, response:{response}, feedback:{feedback}")
                    
        # analyze the result
        try:
            self.sub_problems = json.loads(response)
            print(f"ğŸ”„ Split into {len(self.sub_problems)} sub-problems")
            return self.sub_problems
        except json.JSONDecodeError:
            raise ValueError("Failed to parse sub-problems")
        
        
    def get_raw_answer(self, augmented_context, sub_problem, max_iterations=3):
        """
        è¿­ä»£å¼RAGæ£€ç´¢å¢å¼ºå‡½æ•°
        å‚æ•°ï¼š
            sub_problem: åŸå§‹é—®é¢˜/æŸ¥è¯¢
            max_iterations: æœ€å¤§è¿­ä»£æ¬¡æ•°
        è¿”å›ï¼š
            æœ€ç»ˆæ£€ç´¢åˆ°çš„å¢å¼ºæ–‡æœ¬
        """
        raw_answer = ""
        final_raw_answer = None
        
        prompt_first = f"""You are a physics expert. Please solve a subproblem divided from the initial problem. 
        Please comprehend the following subproblem and context about the whole problem, and try to solve the subproblem step by step.
        
        If you need to perform mathematical calculations, use wolfram language and enclose the calculation expressions within `<wolfram>` and `</wolfram>` tags, for example: `<wolfram>2 + 2</wolfram>`.
        The wolfram engine will automatically compute the result and replace it in the answer.

        If information isn't enough, you should ask for more background information instead of answering the subproblem.
        You should use `<rag>``</rag>` tags around your ask. For example: `<rag>How to solve a first-order linear differential equation?</rag>`.

        Context Information:
        {augmented_context}

        Question: {sub_problem}

        Please think step-by-step and use the `<wolfram>``</wolfram>` tags when calculations are needed.
        Your response:"""

        raw_answer = self.model.generate(prompt_first, max_tokens=1000)

        pattern = r'<rag>(.*?)</rag>'
    
        # ä½¿ç”¨ re.search æŸ¥æ‰¾ç¬¬ä¸€ä¸ªåŒ¹é…
        match = re.search(pattern, raw_answer)
        
        if match:
            background = rag(k=2, query=match.group(1))
        else:
            final_raw_answer = raw_answer
        
        for idx in range(max_iterations-1):
            if final_raw_answer:
                break
            
            prompt = f"""You are a physics expert. Please use the provided context and background information to answer the following questions step-by-step. 
            If you need to perform mathematical calculations, use wolfram language and enclose the calculation expressions within `<wolfram>` and `</wolfram>` tags, for example: `<wolfram>2 + 2</wolfram>`.
            The wolfram engine will automatically compute the result and replace it in the answer.

            Context Information:
            {augmented_context}

            Relevant Background Knowledge:
            {background}

            Question: {sub_problem}

            Please think step-by-step and use the `<wolfram>``</wolfram>` tags when calculations are needed.
            Your response:"""

            raw_answer = self.model.generate(prompt, max_tokens=1000)

            match = re.search(pattern, raw_answer)
        
            if match:
                background += rag(k=2, query=match.group(1))
            else:
                final_raw_answer = raw_answer
            
        return final_raw_answer
    
    # é˜¶æ®µ2: é¡ºåºè§£å†³å­é—®é¢˜
    def solve_subproblems(self) -> List[Dict]:
        """ä½¿ç”¨RAGå’ŒMMAæŒ‰é¡ºåºè§£å†³å­é—®é¢˜(æ”¯æŒåŠ¨æ€æ•°å­¦è®¡ç®—)"""
        
        context = OrderedDict()    # ç»´æŠ¤ä¸Šä¸‹æ–‡ä¿¡æ¯
        
        for idx, sub_problem in enumerate(self.sub_problems, 1):
            print(f"ğŸ” Processing sub-problem {idx}/{len(self.sub_problems)}")
            
            # ç»„åˆä¸Šä¸‹æ–‡ä¸å½“å‰é—®é¢˜
            augmented_context = "\n".join([f"{k}: {v}" for k, v in context.items()])
            
            raw_answer = self.get_raw_answer(augmented_context=augmented_context,sub_problem=sub_problem,max_iterations=3)

            # å¤„ç†ç­”æ¡ˆä¸­çš„MMAè®¡ç®—
            processed_answer = self.process_mma_calculations(raw_answer)
            
            # å­˜å‚¨ä¸­é—´ç»“æœ
            self.intermediate_answers.append({
                "step": idx,
                "context": context.copy(),
                "sub_problem": sub_problem,
                "answer": processed_answer,
            })
            
            # æ›´æ–°ä¸Šä¸‹æ–‡ï¼ˆä¿ç•™æœ€è¿‘3æ­¥ï¼‰
            context[f"Step_{idx}"] = f"problem: {sub_problem}\nanswer: {processed_answer}"
            context = self.clean_context(context, keep_last=3)
            # æ•´ç†æ–°å­—å…¸
            updated_context = OrderedDict()
            updated_context["initial problem"] = self.problem

            for k, v in context.items(): # å†æ·»åŠ  context ä¸­çš„å…¶ä»–å†…å®¹
                if k != "initial problem": # é¿å…é‡å¤æ·»åŠ 
                    updated_context[k] = v

            context = deepcopy(updated_context)  # æ›´æ–°åŸå§‹çš„ context å­—å…¸
        
        print("âœ… All sub-problems solved")
        return self.intermediate_answers

    def process_mma_calculations(self, answer: str) -> str:
        """å¤„ç†ç­”æ¡ˆä¸­çš„wolframè®¡ç®—æ ‡ç­¾"""
        def replace_mma_match(match):
            expr = match.group(1).strip()
            try:
                result = self.mma.evaluate(wlexpr(expr))
                return str(result)
            except WolframLanguageException as e:
                return f"[wolfram error: {str(e)}]"
            except Exception as e:
                return f"[system error: {str(e)}]"
        
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢æ‰€æœ‰<wolfram>æ ‡ç­¾
        return re.sub(r'<wolfram>(.*?)</wolfram>', replace_mma_match, answer, flags=re.DOTALL)


    def clean_context(self, context: dict, keep_last: int = 3) -> dict:
        """ä¿æŒä¸Šä¸‹æ–‡ç®€æ´,ä¿ç•™æœ€è¿‘Nä¸ªå­—å…¸"""
        return dict(list(context.items())[-keep_last:])

    #é˜¶æ®µ3ï¼Œç»¼åˆstepç”Ÿæˆfinal answer
    def generate_conclusion(self, max_retries: int = 3) -> str:
        """Generate a structured final conclusion with a self-checking mechanism"""

        formatted_steps = "\n".join([
            f"Step {ans['step']}: {ans['sub_problem']}\n"
            f"Answer: {ans['answer']}\n"
            for ans in self.intermediate_answers
        ])
        
        final_answer = ""
        
        final_prompt = f"""You are a physics expert. Generate a final conclusion for the given problem.
        Please use '<box>' and '</box>' tags around your final answer.
        Example:
        Problem: Find the square root of 144.
        
        Step-by-step Solutions:
        ......
        
        Your final conclusion and answer:
        The square root of 144 is <box>12</box>.
        
        Problem:
        {self.problem}
        
        Step-by-step Solutions:
        {formatted_steps}
        
        Your final conclusion and answer:
        """                
        
        retry_count = 0
        while retry_count < max_retries:
            try:
                final_answer = self.model.generate(final_prompt, max_tokens=1000)
                
                synthesis_prompt = f"""There is a physics problem and its solution.
                Problem:
                {self.problem}

                Step-by-Step Solutions:
                {formatted_steps}
                
                Final Conclusion:
                {final_answer}

                Please first verify the following conditions:
                - All mathematical calculation results are correctly referenced
                - Key data points are complete
                - Main conclusions are self-consistent
                - Final answer is in tag format <box>...</box>
                
                Please respond in JSON format: {{"valid": bool, "issues": [str]}}, 
                If all conditions are satisfied, please set "valid" to true and leave "issues" empty. Otherwise, set "vaild" to false and provide a list of issues to be solved.
                Your response:"""
                
                eval_prompt = self.model.generate(synthesis_prompt, max_tokens=1000)
                
                evaluation = json.loads(self.model.generate(eval_prompt, max_tokens=300))
                
                if evaluation.get("valid", True):
                    self.final_conclusion = final_answer
                    return self.final_conclusion

                # Regenerate with feedback
                final_prompt = f"""You are a physics expert. Generate a final conclusion for the given problem.
                Please use '<box>' and '</box>' tags around your final answer.
                Example:
                Problem: Find the square root of 144.
                
                Step-by-step Solutions:
                ......
                
                Your final conclusion and answer:
                The square root of 144 is <box>12</box>.
                
                Problem:
                {self.problem}
                
                Step-by-step Solutions:
                {formatted_steps}
                
                Issues to be solved:
                {evaluation.get("issues", "No issues found")}
                
                Your final conclusion and answer:
                """                

                retry_count += 1
                
            except (json.JSONDecodeError, KeyError) as e:
                print(f"Conclusion validation error: {str(e)}")
                retry_count += 1
                continue
                
        # Fallback return mechanism
        print("âš ï¸ Conclusion generation reached maximum retries, returning the last result")
        self.final_conclusion = final_answer
        return self.final_conclusion

# ä¸»æ‰§è¡Œæµç¨‹
def main(model_path):
    executor = WorkflowExecutor(model_path)
    
    # Step 1: è¾“å…¥é—®é¢˜
    problem = input("please load your problem")
    if not problem.strip():
        raise ValueError("Problem input cannot be empty")
    
    executor.problem = problem
    
    # Step 2: æ‹†åˆ†é—®é¢˜
    sub_problems = executor.split_into_subproblems()
    print("\nsplit sub-problems:")
    print("\n".join(f"{i+1}. {p}" for i, p in enumerate(sub_problems)))
    
    # Step 3: è§£å†³å­é—®é¢˜
    answers = executor.solve_subproblems()
    print("\nintermediate answers:")
    print(json.dumps(answers, indent=4, ensure_ascii=False))
    
    # Step 4: ç”Ÿæˆç»“è®º
    conclusion = executor.generate_conclusion(max_retries = 4)
    print("\nfinal conclusions:")
    print(conclusion)
            

if __name__ == "__main__":
    main('/data/public/models/DeepSeek-R1-Distill-Qwen-14B')